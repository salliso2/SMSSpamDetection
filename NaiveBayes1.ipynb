{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "747 of 5572 messages are spam: 13.406317300789663%\n",
      "Shape of counts vector: (5572, 8672)\n",
      " Prediction: [1 0]\n",
      "Success Rate: 99.03086862885858%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def eliminateStop(words, stop_words):\n",
    "    returnList = [w for w in words if not w in stop_words]\n",
    "    return returnList\n",
    "\n",
    "def main():\n",
    "    # Read In Data and Format It Better\n",
    "    df = pd.read_csv('spam.csv', sep=',', encoding='ISO-8859-1')\n",
    "    df = df.drop(columns=[ \"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "\n",
    "    # Add 2 New Columns: For binary spam/ham indicator and length of message\n",
    "    df['Result']= df['Classification'].map( {'spam' : int(1), 'ham' : int(0)})\n",
    "    df['Message_Size'] = df['Text'].apply(len)\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate Statistics\n",
    "    totalMessages = df['Result'].count()\n",
    "    numSpams = df[df['Result']==1]['Result'].count()\n",
    "    numValid = df[df['Result'] == 0]['Result'].count()\n",
    "   \n",
    "    '''\n",
    "    vocab = []\n",
    "    # Get a total word count for unfiltered data\n",
    "    for mes in df['Text']:\n",
    "        for word in mes.split():\n",
    "            if word.lower() not in vocab:\n",
    "                vocab.append(word.lower())\n",
    "    print(len(vocab))\n",
    "    '''\n",
    "\n",
    "    # Print Distribution\n",
    "    print(f'{numSpams} of {totalMessages} messages are spam: {((numSpams/totalMessages)*100)}%')\n",
    "   \n",
    "    # Tokenize Each Message and add the list of tokens to the DataFrame\n",
    "    df['Tokens'] = df['Text'].apply(word_tokenize)\n",
    "    \n",
    "    # Add Tokenize With Removed Stop Words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    df['Filtered_Tokens'] = df['Tokens'].apply(eliminateStop, args=(stop_words,),)\n",
    "    \n",
    "    '''\n",
    "    vocab2 = []\n",
    "    for entry in df['Filtered_Tokens']:\n",
    "        for tok in entry:\n",
    "            if tok.lower() not in vocab2:\n",
    "                vocab2.append(tok.lower())\n",
    "    print(len(vocab2))\n",
    "    '''\n",
    "    \n",
    "    # Create the Naive Bayes Classifier Object\n",
    "    classifier = MultinomialNB()\n",
    "    \n",
    "    targs = df['Result'].values\n",
    "    # \"Vectorize\" The Messages\n",
    "    vectorizer = CountVectorizer()\n",
    "    counts = vectorizer.fit_transform(df['Text'].values)\n",
    "    \n",
    "    print(f'Shape of counts vector: {counts.get_shape()}')\n",
    "\n",
    "    # Train the data\n",
    "    classifier.fit(counts, targs)\n",
    "    \n",
    "    # Test With Example Data\n",
    "    examples = ['Free things !!! Get it to win big prize!', 'Hello, Susan. Going to game']\n",
    "    examples_counts = vectorizer.transform(examples)\n",
    "    predictions = classifier.predict(examples_counts)\n",
    "    # Should print [1, 0] for (spam, ham)\n",
    "    print(f' Prediction: {predictions}')\n",
    "    \n",
    "    # Test with the new set :)\n",
    "    testDF = pd.read_csv('testset.csv', sep=',', encoding='ISO-8859-1')\n",
    "    testDF['Result']= testDF['type'].map( {'spam' : int(1), 'ham' : int(0)})\n",
    "\n",
    "    predictionsFull = classifier.predict(vectorizer.transform(testDF['text']))\n",
    "    \n",
    "    \n",
    "    countRight = 0\n",
    "    for i in range(len(predictionsFull)):\n",
    "        if (predictionsFull[i] == testDF['Result'][i]):\n",
    "            countRight+=1\n",
    "            \n",
    "    size = df['Result'].size\n",
    "    print(f'Success Rate: {countRight / size * 100}%')\n",
    "    \n",
    "    \n",
    "    # Confusion matrix\n",
    "    print(\"Confusion Matrix: \")\n",
    "    cm = confusion_matrix(testDF['Result'], predictionsFull)\n",
    "    print(cm)\n",
    "    \n",
    "    #Accuracy Score \n",
    "    print(\"Accuracy: \")\n",
    "    print(metrics.accuracy_score(testDF['Result'], predictionsFull))\n",
    "    # Display the confusion matrix\n",
    "   \n",
    "    \n",
    "    \n",
    "if __name__ =='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
